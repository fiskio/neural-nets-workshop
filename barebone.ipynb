{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- setup some useful stuff\n",
    "require 'nn'\n",
    "\n",
    "-- help function to print in green\n",
    "function cprint(str) print(sys.COLORS.green..str..'\\27[0m') end\n",
    "\n",
    "-- tests\n",
    "precision = 1e-5\n",
    "tester = torch.Tester()\n",
    "function runTest(test)\n",
    "    tester:add(test)\n",
    "    tester:run()\n",
    "    tester = torch.Tester()\n",
    "end\n",
    "\n",
    "-- setting the random generator seed\n",
    "torch.manualSeed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookupTable = function(vocabSize, embeddingSize)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Linear Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running 1 tests\t\n",
       "_\r",
       "\r",
       "\r",
       "|  ==> unknown\n",
       "\u001b[0;32mLinearModule weight is now:\u001b[0m\t\n",
       " 2  2\n",
       " 2  2\n",
       " 2  2\n",
       "[torch.DoubleTensor of dimension 3x2]\n",
       "\n",
       "\u001b[0;32mLinearModule bias is now:\u001b[0m\t\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.DoubleTensor of dimension 3x1]\n",
       "\n",
       "\u001b[0;32mTesting input vector\u001b[0m\t\n",
       "-1\n",
       "-1\n",
       "[torch.DoubleTensor of dimension 2]\n",
       "\n",
       "\u001b[0;32mOutput is:\u001b[0m\t"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "-3\n",
       "-3\n",
       "-3\n",
       "[torch.DoubleTensor of dimension 3x1]\n",
       "\n",
       "\u001b[0;32mTesting input matrix\u001b[0m\t\n",
       " 2  2  2\n",
       " 2  2  2\n",
       "[torch.DoubleTensor of dimension 2x3]\n",
       "\n",
       "\u001b[0;32mOutput is:\u001b[0m\t\n",
       " 9  9  9\n",
       " 9  9  9\n",
       " 9  9  9\n",
       "[torch.DoubleTensor of dimension 3x3]\n",
       "\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "              \r",
       "_  ==> Done \n",
       "\n",
       "Completed 2 asserts in 1 tests with 0 errors\t\n",
       "\n",
       "--------------------------------------------------------------------------------\t\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearModule = function(inputSize, outputSize)\n",
    "    \n",
    "    -- module to build\n",
    "    local this = {}\n",
    "    \n",
    "    -- standard deviation for initialization\n",
    "    local stdv =  1./math.sqrt(outputSize)\n",
    "    \n",
    "    -- weight matrix\n",
    "    this.weight = torch.Tensor(outputSize, inputSize):uniform(-stdv, stdv)\n",
    "    \n",
    "    -- bias vector\n",
    "    this.bias = torch.Tensor(outputSize, 1):uniform(-stdv, stdv)\n",
    "    \n",
    "    -- forward operation\n",
    "    this.forward = function(input)\n",
    "        -- quirk to make it work with vectors and matrices\n",
    "        input = (input:dim() == 1) and input:reshape(input:size(1), 1) or input\n",
    "        -- multiply the input and weight matrix\n",
    "        local output = this.weight * input\n",
    "        -- add the expanded bias vector and return\n",
    "        return output + this.bias:expand(output:size())\n",
    "    end\n",
    "\n",
    "    return this\n",
    "end\n",
    "\n",
    "runTest(function()\n",
    "    print()\n",
    "    local layer = linearModule(2,3)\n",
    "    layer.weight:fill(2)\n",
    "    layer.bias:fill(1)\n",
    "    cprint('LinearModule weight is now:')\n",
    "    print(layer.weight)\n",
    "    cprint('LinearModule bias is now:')\n",
    "    print(layer.bias)\n",
    "\n",
    "    local inputVector = torch.Tensor(2):fill(-1)\n",
    "    cprint('Testing input vector')\n",
    "    print(inputVector)\n",
    "    local output = layer.forward(inputVector)\n",
    "    cprint('Output is:')\n",
    "    print(output)\n",
    "    local expected = torch.Tensor(3,1):fill(-3)\n",
    "    tester:assertTensorEq(expected, output, precision)\n",
    "\n",
    "    local inputMatrix = torch.Tensor(2, 3):fill(2)\n",
    "    cprint('Testing input matrix')\n",
    "    print(inputMatrix)\n",
    "    local output = layer.forward(inputMatrix)\n",
    "    cprint('Output is:')\n",
    "    print(output)\n",
    "    local expected = torch.Tensor(3,3):fill(9)\n",
    "    tester:assertTensorEq(expected, output, precision)\n",
    "end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid = function(input)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogSoftMax\n",
    "\\begin{equation*}\n",
    "   LogSoftMax(x_i) = -\\ln \\Bigl(\\frac{1}{e^{x_i}} \\sum_j e^{x_j}\\Bigr)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running 1 tests\t\n",
       "_\r",
       "\r",
       "\r",
       "|  ==> unknown\n",
       "\u001b[0;32mInput matrix is:\u001b[0m\t\n",
       " 1  2  3\n",
       " 4  5  6\n",
       " 7  8  9\n",
       "[torch.DoubleTensor of dimension 3x3]\n",
       "\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mOutput from nn.LogSoftMax is:\u001b[0m\t\n",
       "-2.4076 -1.4076 -0.4076\n",
       "-2.4076 -1.4076 -0.4076\n",
       "-2.4076 -1.4076 -0.4076\n",
       "[torch.DoubleTensor of dimension 3x3]\n",
       "\n",
       "\u001b[0;32mOutput from logSoftMax is:\u001b[0m\t\n",
       "-2.4076 -1.4076 -0.4076\n",
       "-2.4076 -1.4076 -0.4076\n",
       "-2.4076 -1.4076 -0.4076\n",
       "[torch.DoubleTensor of dimension 3x3]\n",
       "\n",
       "\r",
       "              \r",
       "_  ==> Done \n",
       "\n",
       "Completed 1 asserts in 1 tests with 0 errors\t\n",
       "\n",
       "--------------------------------------------------------------------------------\t\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logSoftMax = function(input) \n",
    "    \n",
    "    -- quirk to make it work with vectors and matrices\n",
    "    input = (input:dim() == 1) and input:reshape(input:size(1), 1) or input\n",
    "    \n",
    "    -- calculate sum of e^x_i and expand it to the right size\n",
    "    local sumOfExp = torch.exp(input):sum(2):expand(input:size())\n",
    "    \n",
    "    -- calculate the rest of the formula and return\n",
    "    return torch.exp(input):pow(-1):cmul(sumOfExp):log():mul(-1)\n",
    "end\n",
    "\n",
    "\n",
    "runTest(function()\n",
    "    print()\n",
    "    input = torch.range(1,9):reshape(3,3)\n",
    "    cprint('Input matrix is:')\n",
    "    print(input)\n",
    "    local lms = nn.LogSoftMax()\n",
    "    local expected = lms:forward(input)\n",
    "    cprint('Output from nn.LogSoftMax is:')\n",
    "    print(expected)\n",
    "    local output = logSoftMax(input)\n",
    "    cprint('Output from logSoftMax is:')\n",
    "    print(output)\n",
    "    tester:assertTensorEq(expected, output, 1e-5)\n",
    "end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mLookupTable of 5 words, each is a vector of size 2\u001b[0m\t\n",
       "-0.6836  0.2986  0.0171  0.4377 -0.3868\n",
       "-0.2140  0.2053 -0.5711 -0.4605  0.6230\n",
       "[torch.DoubleTensor of dimension 2x5]\n",
       "\n",
       " 0.2700\n",
       "-0.1449\n",
       "[torch.DoubleTensor of dimension 2x1]\n",
       "\n",
       "\u001b[0;32mContext-To-Hidden matrix is:\u001b[0m\t\n",
       "-0.0925  0.0145  0.3566  0.2757 -0.2960  0.1435\n",
       "-0.1298  0.1921 -0.3156 -0.2375  0.3468  0.0338\n",
       " 0.3081  0.1599 -0.1976 -0.2216  0.1306 -0.2654\n",
       " 0.2590  0.3937  0.0451  0.0136  0.0242 -0.1953\n",
       "-0.2108  0.4052 -0.3322  0.3800  0.3243  0.0476\n",
       " 0.3269  0.3124  0.1087 -0.2542 -0.1314 -0.1806\n",
       "[torch.DoubleTensor of dimension 6x6]\n",
       "\n",
       "-0.1231\n",
       " 0.1636\n",
       " 0.1845\n",
       " 0.2830\n",
       " 0.3242\n",
       " 0.2909\n",
       "[torch.DoubleTensor of dimension 6x1]\n",
       "\n",
       "\u001b[0;32mHidden-To-Embedding matrix is:\u001b[0m\t\n",
       " 0.7742 -0.1910  0.5598  0.7755  0.2841  0.7019\n",
       "-0.8317  0.8713 -0.6767  0.5707  0.7971  0.3380\n",
       "[torch.FloatTensor of dimension 2x6]\n",
       "\n",
       "\u001b[0;32mEmbedding-To-Vocabulary matrix is:\u001b[0m\t\n",
       " 0.2129  0.1614\n",
       "-0.9816 -0.2554\n",
       "-0.7971  0.8803\n",
       " 0.3270  0.9473\n",
       "-0.9899 -0.4322\n",
       "[torch.FloatTensor of dimension 5x2]\n",
       "\n",
       "\u001b[0;32mContext will be built of words at indices...\u001b[0m\t\n",
       " 1\n",
       " 3\n",
       " 5\n",
       "[torch.LongTensor of dimension 3]\n",
       "\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "[string \"-- network parameters...\"]:49: attempt to call method 'index' (a nil value)\nstack traceback:\n\t[string \"-- network parameters...\"]:49: in main chunk\n\t[C]: in function 'xpcall'\n\t/home/fiskio/torch/install/share/lua/5.1/itorch/main.lua:174: in function </home/fiskio/torch/install/share/lua/5.1/itorch/main.lua:140>\n\t/home/fiskio/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/home/fiskio/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/home/fiskio/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/home/fiskio/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/home/fiskio/torch/install/share/lua/5.1/itorch/main.lua:341: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x00406170",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"-- network parameters...\"]:49: attempt to call method 'index' (a nil value)\nstack traceback:\n\t[string \"-- network parameters...\"]:49: in main chunk\n\t[C]: in function 'xpcall'\n\t/home/fiskio/torch/install/share/lua/5.1/itorch/main.lua:174: in function </home/fiskio/torch/install/share/lua/5.1/itorch/main.lua:140>\n\t/home/fiskio/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/home/fiskio/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/home/fiskio/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/home/fiskio/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/home/fiskio/torch/install/share/lua/5.1/itorch/main.lua:341: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x00406170"
     ]
    }
   ],
   "source": [
    "-- network parameters\n",
    "local embeddingSize = 2\n",
    "local contextLength = 3\n",
    "local vocabSize = 5\n",
    "local hiddenSize = 6\n",
    "local stdv = 1\n",
    "\n",
    "-- IndexToEmbedding\n",
    "local lookupTable = linearModule(vocabSize, embeddingSize)\n",
    "cprint('LookupTable of '..vocabSize..' words, each is a vector of size '..embeddingSize)\n",
    "print(lookupTable.weight)\n",
    "print(lookupTable.bias)\n",
    "\n",
    "-- ContextToHidden\n",
    "local contextToHidden = linearModule(contextLength * embeddingSize, hiddenSize)\n",
    "cprint('Context-To-Hidden matrix is:')\n",
    "print(contextToHidden.weight)\n",
    "print(contextToHidden.bias)\n",
    "\n",
    "local hiddenToEmbedding = torch.FloatTensor(embeddingSize, hiddenSize):uniform(-stdv, stdv)\n",
    "\n",
    "cprint('Hidden-To-Embedding matrix is:')\n",
    "print(hiddenToEmbedding)\n",
    "\n",
    "local embeddingToVocabulary = torch.FloatTensor(vocabSize, embeddingSize):uniform(-stdv, stdv)\n",
    "\n",
    "cprint('Embedding-To-Vocabulary matrix is:')\n",
    "print(embeddingToVocabulary)\n",
    "\n",
    "function softMax(matrix)\n",
    "   -- -log(sum(exp(matrix)) * 1/exp(matrix))\n",
    "   return torch.mul(torch.exp(matrix):pow(-1), torch.sum(torch.exp(matrix), 1)[1]):log():mul(-1)\n",
    "end\n",
    "\n",
    "-- Forward\n",
    "--[[\n",
    "local oneHot = torch.FloatTensor():eye(vocabSize)\n",
    "cprint('1-Hot representation of second word is:')\n",
    "print(oneHot[2])\n",
    "\n",
    "local secondWord = torch.mv(lookupTable, oneHot[2])\n",
    "cprint('Vector representation of second word is:')\n",
    "print(secondWord)\n",
    "--]]\n",
    "local oneHotIndices = torch.LongTensor{1,3,5}\n",
    "cprint('Context will be built of words at indices...')\n",
    "print(oneHotIndices)\n",
    "\n",
    "local contextMatrix = lookupTable:index(2, oneHotIndices)\n",
    "cprint('...which corresponds to the following matrix:')\n",
    "print(oneHotContext)\n",
    "--[[\n",
    "local contextMatrix = torch.mm(lookupTable, oneHotContext)\n",
    "cprint('The corresponding matrix of embeddings are:')\n",
    "print(contextMatrix)\n",
    "--]]\n",
    "local contextVector = torch.reshape(contextMatrix, contextMatrix:nElement())\n",
    "cprint('...which reshaped as a vector is:')\n",
    "print(contextVector)\n",
    "\n",
    "local output = contextToHidden * contextVector\n",
    "print(output)\n",
    "\n",
    "output = torch.mv(hiddenToEmbedding, output)\n",
    "print(output)\n",
    "\n",
    "output = torch.mv(embeddingToVocabulary, output)\n",
    "print(output)\n",
    "\n",
    "output = softMax(output)\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
