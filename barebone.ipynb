{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- setup some useful stuff\n",
    "require 'nn'\n",
    "\n",
    "-- help function to print in green\n",
    "function cprint(str) print(sys.COLORS.green..str..'\\27[0m') end\n",
    "\n",
    "-- tests\n",
    "precision = 1e-5\n",
    "tester = torch.Tester()\n",
    "function runTest(test)\n",
    "    tester:add(test)\n",
    "    tester:run()\n",
    "    tester = torch.Tester()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Linear Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running 1 tests\t\n",
       "_\r",
       "\r",
       "\r",
       "|  ==> unknown\n",
       "\u001b[0;32mLinearModule weight is now:\u001b[0m\t\n",
       " 2  2\n",
       " 2  2\n",
       " 2  2\n",
       "[torch.DoubleTensor of dimension 3x2]\n",
       "\n",
       "\u001b[0;32mLinearModule bias is now:\u001b[0m\t\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.DoubleTensor of dimension 3x1]\n",
       "\n",
       "\u001b[0;32mTesting input vector\u001b[0m\t\n",
       "-1\n",
       "-1\n",
       "[torch.DoubleTensor of dimension 2]\n",
       "\n",
       "\u001b[0;32mOutput is:\u001b[0m\t\n",
       "-3\n",
       "-3\n",
       "-3\n",
       "[torch.DoubleTensor of dimension 3x1]\n",
       "\n",
       "\u001b[0;32mTesting input matrix\u001b[0m\t\n",
       " 2  2  2\n",
       " 2  2  2\n",
       "[torch.DoubleTensor of dimension 2x3]\n",
       "\n",
       "\u001b[0;32mOutput is:\u001b[0m\t\n",
       " 9  9  9\n",
       " 9  9  9\n",
       " 9  9  9\n",
       "[torch.DoubleTensor of dimension 3x3]\n",
       "\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "              \r",
       "_  ==> Done \n",
       "\n",
       "Completed 2 asserts in 1 tests with 0 errors\t\n",
       "\n",
       "--------------------------------------------------------------------------------\t\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearModule = function(inputSize, outputSize)\n",
    "    \n",
    "    -- module to build\n",
    "    local this = {}\n",
    "    \n",
    "    -- standard deviation for initialization\n",
    "    local stdv =  1./math.sqrt(outputSize)\n",
    "    \n",
    "    -- weight matrix\n",
    "    this.weight = torch.Tensor(outputSize, inputSize):uniform(-stdv, stdv)\n",
    "    \n",
    "    -- bias vector\n",
    "    this.bias = torch.Tensor(outputSize, 1):uniform(-stdv, stdv)\n",
    "    \n",
    "    -- forward operation\n",
    "    this.forward = function(input)\n",
    "        -- quirk to make it work with vectors and matrices\n",
    "        input = (input:dim() == 1) and input:reshape(input:size(1), 1) or input\n",
    "        -- multiply the input and weight matrix\n",
    "        local output = this.weight * input\n",
    "        -- add the expanded bias vector and return\n",
    "        return output + this.bias:expand(output:size())\n",
    "    end\n",
    "\n",
    "    return this\n",
    "end\n",
    "\n",
    "runTest(function()\n",
    "    print()\n",
    "    local layer = linearModule(2,3)\n",
    "    layer.weight:fill(2)\n",
    "    layer.bias:fill(1)\n",
    "    cprint('LinearModule weight is now:')\n",
    "    print(layer.weight)\n",
    "    cprint('LinearModule bias is now:')\n",
    "    print(layer.bias)\n",
    "\n",
    "    local inputVector = torch.Tensor(2):fill(-1)\n",
    "    cprint('Testing input vector')\n",
    "    print(inputVector)\n",
    "    local output = layer.forward(inputVector)\n",
    "    cprint('Output is:')\n",
    "    print(output)\n",
    "    local expected = torch.Tensor(3,1):fill(-3)\n",
    "    tester:assertTensorEq(expected, output, precision)\n",
    "\n",
    "    local inputMatrix = torch.Tensor(2, 3):fill(2)\n",
    "    cprint('Testing input matrix')\n",
    "    print(inputMatrix)\n",
    "    local output = layer.forward(inputMatrix)\n",
    "    cprint('Output is:')\n",
    "    print(output)\n",
    "    local expected = torch.Tensor(3,3):fill(9)\n",
    "    tester:assertTensorEq(expected, output, precision)\n",
    "end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogSoftMax\n",
    "\\begin{equation*}\n",
    "   LogSoftMax(x_i) = -\\ln \\Bigl(\\frac{1}{e^{x_i}} \\sum_j e^{x_j}\\Bigr)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running 1 tests\t\n",
       "_\r",
       "\r",
       "\r",
       "|  ==> unknown\n",
       "\u001b[0;32mInput matrix is:\u001b[0m\t\n",
       " 1  2  3\n",
       " 4  5  6\n",
       " 7  8  9\n",
       "[torch.DoubleTensor of dimension 3x3]\n",
       "\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mOutput from nn.LogSoftMax is:\u001b[0m\t\n",
       "-2.4076 -1.4076 -0.4076\n",
       "-2.4076 -1.4076 -0.4076\n",
       "-2.4076 -1.4076 -0.4076\n",
       "[torch.DoubleTensor of dimension 3x3]\n",
       "\n",
       "\u001b[0;32mOutput from logSoftMax is:\u001b[0m\t\n",
       "-2.4076 -1.4076 -0.4076\n",
       "-2.4076 -1.4076 -0.4076\n",
       "-2.4076 -1.4076 -0.4076\n",
       "[torch.DoubleTensor of dimension 3x3]\n",
       "\n",
       "\r",
       "              \r",
       "_  ==> Done \n",
       "\n",
       "Completed 1 asserts in 1 tests with 0 errors\t\n",
       "\n",
       "--------------------------------------------------------------------------------\t\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logSoftMax = function(input) \n",
    "    \n",
    "    -- quirk to make it work with vectors and matrices\n",
    "    input = (input:dim() == 1) and input:reshape(input:size(1), 1) or input\n",
    "    \n",
    "    -- calculate sum of e^x_i and expand it to the right size\n",
    "    local sumOfExp = torch.exp(input):sum(2):expand(input:size())\n",
    "    \n",
    "    -- calculate the rest of the formula and return\n",
    "    return torch.exp(input):pow(-1):cmul(sumOfExp):log():mul(-1)\n",
    "end\n",
    "\n",
    "\n",
    "runTest(function()\n",
    "    print()\n",
    "    input = torch.range(1,9):reshape(3,3)\n",
    "    cprint('Input matrix is:')\n",
    "    print(input)\n",
    "    local lms = nn.LogSoftMax()\n",
    "    local expected = lms:forward(input)\n",
    "    cprint('Output from nn.LogSoftMax is:')\n",
    "    print(expected)\n",
    "    local output = logSoftMax(input)\n",
    "    cprint('Output from logSoftMax is:')\n",
    "    print(output)\n",
    "    tester:assertTensorEq(expected, output, 1e-5)\n",
    "end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mLookupTable of 10 words, each is a vector of size 4\u001b[0m\t\n",
       "-0.2509  0.5931  0.9014 -0.6331  0.4640  0.5594  0.1973  0.1937 -0.6880 -0.1083\n",
       "-0.6880 -0.8001 -0.8838 -0.0815  0.7324 -0.3326  0.2022 -0.7143  0.4161  0.3018\n",
       "-0.9588 -0.8872  0.9398  0.4440  0.6649  0.8771 -0.5753 -0.9984 -0.6364  0.9844\n",
       "-0.6332  0.2350 -0.3915  0.2233  0.0495 -0.9859 -0.1361 -0.9539 -0.4175  0.0495\n",
       "[torch.FloatTensor of dimension 4x10]\n",
       "\n",
       "\u001b[0;32mContext-To-Hidden matrix is:\u001b[0m\t\n",
       "Columns 1 to 10\n",
       " 0.2237 -0.2003 -0.7210 -0.9067 -0.4157  0.9475 -0.2673 -0.5345 -0.0879 -0.8188\n",
       "-0.6007 -0.2351  0.0285  0.9665  0.1848 -0.0665 -0.9071  0.7199  0.2151  0.3606\n",
       "-0.8699 -0.9735  0.8978  0.8844  0.9313  0.1266  0.6168 -0.2292 -0.3908 -0.9681\n",
       " 0.3685 -0.5179 -0.1197  0.3665 -0.7559  0.2200 -0.0096  0.6664 -0.9312 -0.6533\n",
       "-0.4824 -0.6355  0.3250  0.5107 -0.3766 -0.1497  0.0401 -0.5841  0.0934  0.1354\n",
       "\n",
       "Columns 11 to 12\n",
       " 0.5704  0.2368\n",
       "-0.6590 -0.0990\n",
       "-0.8047 -0.5382\n",
       " 0.8186 -0.2179\n",
       "-0.6303 -0.9374\n",
       "[torch.FloatTensor of dimension 5x12]\n",
       "\n",
       "\u001b[0;32mHidden-To-Embedding matrix is:\u001b[0m\t\n",
       " 0.9392  0.6846  0.5503 -0.1005  0.8790\n",
       "-0.2097  0.7897  0.8533  0.1958  0.4545\n",
       " 0.8437 -0.3469 -0.8230  0.1409 -0.6080\n",
       " 0.0417 -0.9095  0.9223 -0.3493  0.6891\n",
       "[torch.FloatTensor of dimension 4x5]\n",
       "\n",
       "\u001b[0;32mEmbedding-To-Vocabulary matrix is:\u001b[0m\t\n",
       "-0.2226  0.4946 -0.4573  0.0794\n",
       " 0.6575  0.1735 -0.2865  0.9305\n",
       "-0.4381  0.2141  0.0854 -0.4480\n",
       "-0.7182 -0.4075  0.6044 -0.6695\n",
       "-0.8509 -0.9687  0.9738 -0.1532\n",
       " 0.5445 -0.2102 -0.6026 -0.4130\n",
       "-0.9890 -0.9718  0.6309 -0.6023\n",
       " 0.4137  0.4227  0.4580  0.5804\n",
       " 0.5425  0.2119 -0.8519  0.8526\n",
       "-0.2831  0.3022 -0.7683  0.8299\n",
       "[torch.FloatTensor of dimension 10x4]\n",
       "\n",
       "\u001b[0;32mContext will be built of words at indices...\u001b[0m\t\n",
       " 1\n",
       " 3\n",
       " 5\n",
       "[torch.LongTensor of dimension 3]\n",
       "\n",
       "\u001b[0;32m...which corresponds to the following matrix:\u001b[0m\t\n",
       "nil\t\n",
       "\u001b[0;32m...which reshaped as a vector is:\u001b[0m\t\n",
       "-0.2509\n",
       " 0.9014\n",
       " 0.4640\n",
       "-0.6880\n",
       "-0.8838\n",
       " 0.7324\n",
       "-0.9588\n",
       " 0.9398\n",
       " 0.6649\n",
       "-0.6332\n",
       "-0.3915\n",
       " 0.0495\n",
       "[torch.FloatTensor of dimension 12]\n",
       "\n",
       " 1.1164\n",
       " 0.7891\n",
       "-1.7468\n",
       " 0.0609\n",
       "-0.8399\n",
       "[torch.FloatTensor of dimension 5]\n",
       "\n",
       "-0.1169\n",
       "-1.4714\n",
       " 2.6251\n",
       "-2.8824\n",
       "[torch.FloatTensor of dimension 4]\n",
       "\n",
       "-2.1310\n",
       "-3.7663\n",
       " 1.2517\n",
       " 4.1997\n",
       " 4.5227\n",
       "-0.1456\n",
       " 4.9379\n",
       "-1.1408\n",
       "-5.0691\n",
       "-4.8204\n",
       "[torch.FloatTensor of dimension 10]\n",
       "\n",
       " -7.8450\n",
       " -9.4802\n",
       " -4.4622\n",
       " -1.5142\n",
       " -1.1913\n",
       " -5.8596\n",
       " -0.7760\n",
       " -6.8547\n",
       "-10.7830\n",
       "-10.5343\n",
       "[torch.FloatTensor of dimension 10]\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "--- setting the random generator seed for reproducibility\n",
    "torch.manualSeed(42)\n",
    "\n",
    "-- network parameters\n",
    "local embeddingSize = 4\n",
    "local contextLength = 3\n",
    "local vocabSize = 10\n",
    "local hiddenSize = 5\n",
    "local stdv = 1\n",
    "-- Vocabulary\n",
    "\n",
    "local lookupTable = torch.FloatTensor(embeddingSize, vocabSize):uniform(-stdv, stdv)\n",
    "cprint('LookupTable of '..vocabSize..' words, each is a vector of size '..embeddingSize)\n",
    "print(lookupTable)\n",
    "\n",
    "-- Hidden\n",
    "\n",
    "local contextToHidden = torch.FloatTensor(hiddenSize, contextLength * embeddingSize):uniform(-stdv, stdv)\n",
    "\n",
    "cprint('Context-To-Hidden matrix is:')\n",
    "print(contextToHidden)\n",
    "\n",
    "local hiddenToEmbedding = torch.FloatTensor(embeddingSize, hiddenSize):uniform(-stdv, stdv)\n",
    "\n",
    "cprint('Hidden-To-Embedding matrix is:')\n",
    "print(hiddenToEmbedding)\n",
    "\n",
    "local embeddingToVocabulary = torch.FloatTensor(vocabSize, embeddingSize):uniform(-stdv, stdv)\n",
    "\n",
    "cprint('Embedding-To-Vocabulary matrix is:')\n",
    "print(embeddingToVocabulary)\n",
    "\n",
    "function softMax(matrix)\n",
    "   -- -log(sum(exp(matrix)) * 1/exp(matrix))\n",
    "   return torch.mul(torch.exp(matrix):pow(-1), torch.sum(torch.exp(matrix), 1)[1]):log():mul(-1)\n",
    "end\n",
    "\n",
    "-- Forward\n",
    "--[[\n",
    "local oneHot = torch.FloatTensor():eye(vocabSize)\n",
    "cprint('1-Hot representation of second word is:')\n",
    "print(oneHot[2])\n",
    "\n",
    "local secondWord = torch.mv(lookupTable, oneHot[2])\n",
    "cprint('Vector representation of second word is:')\n",
    "print(secondWord)\n",
    "--]]\n",
    "local oneHotIndices = torch.LongTensor{1,3,5}\n",
    "cprint('Context will be built of words at indices...')\n",
    "print(oneHotIndices)\n",
    "\n",
    "local contextMatrix = lookupTable:index(2, oneHotIndices)\n",
    "cprint('...which corresponds to the following matrix:')\n",
    "print(oneHotContext)\n",
    "--[[\n",
    "local contextMatrix = torch.mm(lookupTable, oneHotContext)\n",
    "cprint('The corresponding matrix of embeddings are:')\n",
    "print(contextMatrix)\n",
    "--]]\n",
    "local contextVector = torch.reshape(contextMatrix, contextMatrix:nElement())\n",
    "cprint('...which reshaped as a vector is:')\n",
    "print(contextVector)\n",
    "\n",
    "local output = contextToHidden * contextVector\n",
    "print(output)\n",
    "\n",
    "output = torch.mv(hiddenToEmbedding, output)\n",
    "print(output)\n",
    "\n",
    "output = torch.mv(embeddingToVocabulary, output)\n",
    "print(output)\n",
    "\n",
    "output = softMax(output)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
