{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linearModule = function(inputSize, outputSize)\n",
    "    local matrix = torch.FloatTensor(outputSize, inputSize)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogSoftMax\n",
    "\\begin{equation*}\n",
    "   LogSoftMax(x_i) = -\\ln \\Bigl(\\frac{1}{e^{x_i}} \\sum_j e^{x_j}\\Bigr)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mLookupTable of 10 words, each is a vector of size 4\u001b[0m\t\n",
       "-0.2509  0.5931  0.9014 -0.6331  0.4640  0.5594  0.1973  0.1937 -0.6880 -0.1083\n",
       "-0.6880 -0.8001 -0.8838 -0.0815  0.7324 -0.3326  0.2022 -0.7143  0.4161  0.3018\n",
       "-0.9588 -0.8872  0.9398  0.4440  0.6649  0.8771 -0.5753 -0.9984 -0.6364  0.9844\n",
       "-0.6332  0.2350 -0.3915  0.2233  0.0495 -0.9859 -0.1361 -0.9539 -0.4175  0.0495\n",
       "[torch.FloatTensor of dimension 4x10]\n",
       "\n",
       "\u001b[0;32mContext-To-Hidden matrix is:\u001b[0m\t\n",
       "Columns 1 to 10\n",
       " 0.2237 -0.2003 -0.7210 -0.9067 -0.4157  0.9475 -0.2673 -0.5345 -0.0879 -0.8188\n",
       "-0.6007 -0.2351  0.0285  0.9665  0.1848 -0.0665 -0.9071  0.7199  0.2151  0.3606\n",
       "-0.8699 -0.9735  0.8978  0.8844  0.9313  0.1266  0.6168 -0.2292 -0.3908 -0.9681\n",
       " 0.3685 -0.5179 -0.1197  0.3665 -0.7559  0.2200 -0.0096  0.6664 -0.9312 -0.6533\n",
       "-0.4824 -0.6355  0.3250  0.5107 -0.3766 -0.1497  0.0401 -0.5841  0.0934  0.1354\n",
       "\n",
       "Columns 11 to 12\n",
       " 0.5704  0.2368\n",
       "-0.6590 -0.0990\n",
       "-0.8047 -0.5382\n",
       " 0.8186 -0.2179\n",
       "-0.6303 -0.9374\n",
       "[torch.FloatTensor of dimension 5x12]\n",
       "\n",
       "\u001b[0;32mHidden-To-Embedding matrix is:\u001b[0m\t\n",
       " 0.9392  0.6846  0.5503 -0.1005  0.8790\n",
       "-0.2097  0.7897  0.8533  0.1958  0.4545\n",
       " 0.8437 -0.3469 -0.8230  0.1409 -0.6080\n",
       " 0.0417 -0.9095  0.9223 -0.3493  0.6891\n",
       "[torch.FloatTensor of dimension 4x5]\n",
       "\n",
       "\u001b[0;32mEmbedding-To-Vocabulary matrix is:\u001b[0m\t\n",
       "-0.2226  0.4946 -0.4573  0.0794\n",
       " 0.6575  0.1735 -0.2865  0.9305\n",
       "-0.4381  0.2141  0.0854 -0.4480\n",
       "-0.7182 -0.4075  0.6044 -0.6695\n",
       "-0.8509 -0.9687  0.9738 -0.1532\n",
       " 0.5445 -0.2102 -0.6026 -0.4130\n",
       "-0.9890 -0.9718  0.6309 -0.6023\n",
       " 0.4137  0.4227  0.4580  0.5804\n",
       " 0.5425  0.2119 -0.8519  0.8526\n",
       "-0.2831  0.3022 -0.7683  0.8299\n",
       "[torch.FloatTensor of dimension 10x4]\n",
       "\n",
       "\u001b[0;32mContext will be built of words at indices...\u001b[0m\t\n",
       " 1\n",
       " 3\n",
       " 5\n",
       "[torch.LongTensor of dimension 3]\n",
       "\n",
       "\u001b[0;32m...which corresponds to the following matrix:\u001b[0m\t\n",
       "nil\t\n",
       "\u001b[0;32m...which reshaped as a vector is:\u001b[0m\t\n",
       "-0.2509\n",
       " 0.9014\n",
       " 0.4640\n",
       "-0.6880\n",
       "-0.8838\n",
       " 0.7324\n",
       "-0.9588\n",
       " 0.9398\n",
       " 0.6649\n",
       "-0.6332\n",
       "-0.3915\n",
       " 0.0495\n",
       "[torch.FloatTensor of dimension 12]\n",
       "\n",
       " 1.1164\n",
       " 0.7891\n",
       "-1.7468\n",
       " 0.0609\n",
       "-0.8399\n",
       "[torch.FloatTensor of dimension 5]\n",
       "\n",
       "-0.1169\n",
       "-1.4714\n",
       " 2.6251\n",
       "-2.8824\n",
       "[torch.FloatTensor of dimension 4]\n",
       "\n",
       "-2.1310\n",
       "-3.7663\n",
       " 1.2517\n",
       " 4.1997\n",
       " 4.5227\n",
       "-0.1456\n",
       " 4.9379\n",
       "-1.1408\n",
       "-5.0691\n",
       "-4.8204\n",
       "[torch.FloatTensor of dimension 10]\n",
       "\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       " -7.8450\n",
       " -9.4802\n",
       " -4.4622\n",
       " -1.5142\n",
       " -1.1913\n",
       " -5.8596\n",
       " -0.7760\n",
       " -6.8547\n",
       "-10.7830\n",
       "-10.5343\n",
       "[torch.FloatTensor of dimension 10]\n",
       "\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- help function to print in green\n",
    "function cprint(str) print(sys.COLORS.green..str..'\\27[0m') end\n",
    "\n",
    "--- setting the random generator seed for reproducibility\n",
    "torch.manualSeed(42)\n",
    "\n",
    "-- network parameters\n",
    "local embeddingSize = 4\n",
    "local contextLength = 3\n",
    "local vocabSize = 10\n",
    "local hiddenSize = 5\n",
    "local stdv = 1\n",
    "-- Vocabulary\n",
    "\n",
    "local lookupTable = torch.FloatTensor(embeddingSize, vocabSize):uniform(-stdv, stdv)\n",
    "cprint('LookupTable of '..vocabSize..' words, each is a vector of size '..embeddingSize)\n",
    "print(lookupTable)\n",
    "\n",
    "-- Hidden\n",
    "\n",
    "local contextToHidden = torch.FloatTensor(hiddenSize, contextLength * embeddingSize):uniform(-stdv, stdv)\n",
    "\n",
    "cprint('Context-To-Hidden matrix is:')\n",
    "print(contextToHidden)\n",
    "\n",
    "local hiddenToEmbedding = torch.FloatTensor(embeddingSize, hiddenSize):uniform(-stdv, stdv)\n",
    "\n",
    "cprint('Hidden-To-Embedding matrix is:')\n",
    "print(hiddenToEmbedding)\n",
    "\n",
    "local embeddingToVocabulary = torch.FloatTensor(vocabSize, embeddingSize):uniform(-stdv, stdv)\n",
    "\n",
    "cprint('Embedding-To-Vocabulary matrix is:')\n",
    "print(embeddingToVocabulary)\n",
    "\n",
    "function softMax(matrix)\n",
    "   -- -log(sum(exp(matrix)) * 1/exp(matrix))\n",
    "   return torch.mul(torch.exp(matrix):pow(-1), torch.sum(torch.exp(matrix), 1)[1]):log():mul(-1)\n",
    "end\n",
    "\n",
    "-- Forward\n",
    "--[[\n",
    "local oneHot = torch.FloatTensor():eye(vocabSize)\n",
    "cprint('1-Hot representation of second word is:')\n",
    "print(oneHot[2])\n",
    "\n",
    "local secondWord = torch.mv(lookupTable, oneHot[2])\n",
    "cprint('Vector representation of second word is:')\n",
    "print(secondWord)\n",
    "--]]\n",
    "local oneHotIndices = torch.LongTensor{1,3,5}\n",
    "cprint('Context will be built of words at indices...')\n",
    "print(oneHotIndices)\n",
    "\n",
    "local contextMatrix = lookupTable:index(2, oneHotIndices)\n",
    "cprint('...which corresponds to the following matrix:')\n",
    "print(oneHotContext)\n",
    "--[[\n",
    "local contextMatrix = torch.mm(lookupTable, oneHotContext)\n",
    "cprint('The corresponding matrix of embeddings are:')\n",
    "print(contextMatrix)\n",
    "--]]\n",
    "local contextVector = torch.reshape(contextMatrix, contextMatrix:nElement())\n",
    "cprint('...which reshaped as a vector is:')\n",
    "print(contextVector)\n",
    "\n",
    "local output = contextToHidden * contextVector\n",
    "print(output)\n",
    "\n",
    "output = torch.mv(hiddenToEmbedding, output)\n",
    "print(output)\n",
    "\n",
    "output = torch.mv(embeddingToVocabulary, output)\n",
    "print(output)\n",
    "\n",
    "output = softMax(output)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
